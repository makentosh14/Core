#!/usr/bin/env python3
"""
Updated test with fixed SHORT position logic
"""

def log(message, level="INFO"):
    print(f"[{level}] {message}")

def validate_core_risk_reward(core_candles, direction):
    """
    COMPLETE FIXED VERSION: Both LONG and SHORT positions
    """
    try:
        log(f"üîç DEBUG RR: validate_core_risk_reward called, direction={direction}")
        log(f"üîç DEBUG RR: Available timeframes: {list(core_candles.keys())}")
        
        if '15' not in core_candles:
            log(f"‚ùå DEBUG RR: No '15' timeframe in core_candles")
            return False
        
        candles = core_candles['15'][-20:]
        log(f"üîç DEBUG RR: Got {len(candles)} candles from 15m timeframe")
        
        if len(candles) < 10:
            log(f"‚ùå DEBUG RR: Not enough candles: {len(candles)} < 10")
            return False
        
        highs = [float(c.get('high', 0)) for c in candles]
        lows = [float(c.get('low', 0)) for c in candles]
        closes = [float(c.get('close', 0)) for c in candles]
        
        current_price = closes[-1]
        log(f"üîç DEBUG RR: Current price: {current_price}")
        
        if direction.lower() == "long":
            # FIXED APPROACH FOR LONG POSITIONS
            recent_lows = lows[-10:]
            support_candidate = min(recent_lows)
            max_risk_percent = 0.03
            min_support_level = current_price * (1 - max_risk_percent)
            effective_support = max(support_candidate, min_support_level)
            potential_risk = current_price - effective_support
            
            # Multiple target approaches
            approaches = []
            risk_based_target = current_price + (potential_risk * 1.5)
            approaches.append(("risk_1.5x", risk_based_target))
            
            percentage_target = current_price * 1.04
            approaches.append(("percent_4%", percentage_target))
            
            recent_highs = highs[-10:]
            recent_resistance = max(recent_highs)
            if recent_resistance > current_price * 1.01:
                buffered_resistance = recent_resistance * 1.01
                approaches.append(("resistance_buffered", buffered_resistance))
            
            price_range = max(highs) - min(lows)
            range_target = current_price + (price_range * 0.3)
            approaches.append(("range_30%", range_target))
            
            effective_resistance = max(target for _, target in approaches)
            best_approach = max(approaches, key=lambda x: x[1])
            
            log(f"üîç DEBUG RR: LONG - Support: {effective_support:.4f}")
            log(f"üîç DEBUG RR: LONG - Target approaches: {[(name, f'{target:.4f}') for name, target in approaches]}")
            log(f"üîç DEBUG RR: LONG - Selected: {best_approach[0]} = {effective_resistance:.4f}")
            
            potential_reward = effective_resistance - current_price
            
        else:  # SHORT POSITIONS - IMPROVED LOGIC
            # FIXED APPROACH FOR SHORT POSITIONS - Mirror the LONG logic
            
            # 1. Find proper resistance level
            recent_highs = highs[-10:]  # Last 10 candles
            resistance_candidate = max(recent_highs)
            
            # Cap maximum risk at 3% for safety
            max_risk_percent = 0.03
            max_resistance_level = current_price * (1 + max_risk_percent)
            effective_resistance = min(resistance_candidate, max_resistance_level)
            
            # 2. Calculate risk first
            potential_risk = effective_resistance - current_price
            
            # 3. FIXED TARGET CALCULATION - Multiple approaches for SHORT
            approaches = []
            
            # Approach A: Risk-based target (1.5x risk for 1.5 R/R ratio)
            risk_based_target = current_price - (potential_risk * 1.5)
            approaches.append(("risk_1.5x", risk_based_target))
            
            # Approach B: Percentage-based target (minimum 4% below current)
            percentage_target = current_price * 0.96
            approaches.append(("percent_4%", percentage_target))
            
            # Approach C: Recent support with buffer
            recent_lows = lows[-10:]
            recent_support = min(recent_lows)
            if recent_support < current_price * 0.99:  # At least 1% below
                buffered_support = recent_support * 0.99  # Subtract 1% buffer
                approaches.append(("support_buffered", buffered_support))
            
            # Approach D: Price range expansion
            price_range = max(highs) - min(lows)
            range_target = current_price - (price_range * 0.3)  # 30% of range down
            approaches.append(("range_30%", range_target))
            
            # Choose the lowest reasonable target (for shorts, lower is better)
            valid_approaches = [(name, target) for name, target in approaches if target > 0]
            if not valid_approaches:
                log(f"‚ùå DEBUG RR: No valid SHORT targets found")
                return False
            
            effective_support = min(target for _, target in valid_approaches)
            best_approach = min(valid_approaches, key=lambda x: x[1])
            
            log(f"üîç DEBUG RR: SHORT - Resistance: {effective_resistance:.4f}")
            log(f"üîç DEBUG RR: SHORT - Target approaches: {[(name, f'{target:.4f}') for name, target in valid_approaches]}")
            log(f"üîç DEBUG RR: SHORT - Selected: {best_approach[0]} = {effective_support:.4f}")
            
            potential_reward = current_price - effective_support
        
        log(f"üîç DEBUG RR: Potential reward: {potential_reward:.4f}")
        log(f"üîç DEBUG RR: Potential risk: {potential_risk:.4f}")
        
        if potential_reward <= 0 or potential_risk <= 0:
            log(f"‚ùå DEBUG RR: Invalid reward/risk: {potential_reward:.4f}/{potential_risk:.4f}")
            return False
        
        rr_ratio = potential_reward / potential_risk
        log(f"üîç DEBUG RR: Risk/Reward ratio: {rr_ratio:.3f} (needs >= 1.2)")
        
        # Additional quality checks - IMPROVED FOR ALL ASSET TYPES
        min_reward_threshold = current_price * 0.01  # Reduced to 1% (was 1.5%)
        if potential_reward < min_reward_threshold:
            log(f"‚ùå DEBUG RR: Reward too small: {potential_reward:.4f} < {min_reward_threshold:.4f}")
            return False
        
        max_risk_threshold = current_price * 0.04  # Maximum 4% risk
        if potential_risk > max_risk_threshold:
            log(f"‚ùå DEBUG RR: Risk too large: {potential_risk:.4f} > {max_risk_threshold:.4f}")
            return False
        
        result = rr_ratio >= 1.2
        log(f"üîç DEBUG RR: Risk/reward validation result: {result}")
        
        return result
        
    except Exception as e:
        log(f"‚ùå DEBUG RR: Risk/reward validation error: {e}", level="ERROR")
        import traceback
        log(f"‚ùå DEBUG RR: Traceback: {traceback.format_exc()}", level="ERROR")
        return False

def create_test_cases():
    """Create improved test cases"""
    
    # Case 1: BTCUSDT-like scenario
    btc_candles = []
    base_price = 50000
    for i in range(20):
        price = base_price + (i * 100) + (i % 3 * 50)
        btc_candles.append({
            'high': str(price + 60),
            'low': str(price - 50),
            'close': str(price),
            'open': str(price - 10)
        })
    
    # Case 2: ETHUSDT-like scenario
    eth_candles = []
    base_price = 3000
    for i in range(15):
        price = base_price + (i * 20)
        eth_candles.append({
            'high': str(price + 20),
            'low': str(price - 15),
            'close': str(price),
            'open': str(price - 5)
        })
    
    # Case 3: IMPROVED ADA SHORT scenario - create proper downtrend
    ada_candles = []
    base_price = 1.0
    for i in range(15):
        # Create a clearer downtrend with better range
        if i < 5:  # Resistance area
            price = base_price - (i * 0.005)  # 1.00 to 0.98
        elif i < 10:  # Breakdown
            price = base_price - 0.05 - (i * 0.02)  # 0.95 to 0.85
        else:  # Current area
            price = base_price - 0.15 - (i % 3 * 0.005)  # Around 0.85
            
        ada_candles.append({
            'high': str(price + 0.01),
            'low': str(price - 0.01),
            'close': str(price),
            'open': str(price + 0.005)
        })
    
    return {
        'BTCUSDT': {'15': btc_candles},
        'ETHUSDT': {'15': eth_candles}, 
        'ADAUSDT': {'15': ada_candles}
    }

def run_tests():
    """Run the complete fixed validation tests"""
    print("üß™ TESTING COMPLETE FIXED RISK/REWARD VALIDATION")
    print("=" * 80)
    
    test_cases = create_test_cases()
    
    print("\nüìà TESTING LONG POSITIONS:")
    print("-" * 50)
    
    btc_result = validate_core_risk_reward(test_cases['BTCUSDT'], "Long")
    print(f"‚úì BTCUSDT Long: {'‚úÖ PASS' if btc_result else '‚ùå FAIL'}")
    
    print("\n" + "-" * 50)
    eth_result = validate_core_risk_reward(test_cases['ETHUSDT'], "Long") 
    print(f"‚úì ETHUSDT Long: {'‚úÖ PASS' if eth_result else '‚ùå FAIL'}")
    
    print("\nüìâ TESTING SHORT POSITION (FIXED):")
    print("-" * 50)
    
    ada_result = validate_core_risk_reward(test_cases['ADAUSDT'], "Short")
    print(f"‚úì ADAUSDT Short: {'‚úÖ PASS' if ada_result else '‚ùå FAIL'}")
    
    print("\n" + "=" * 80)
    print("üéØ SUMMARY:")
    print(f"   BTCUSDT Long: {'WORKING ‚úÖ' if btc_result else 'Still failing ‚ùå'}")
    print(f"   ETHUSDT Long: {'WORKING ‚úÖ' if eth_result else 'Still failing ‚ùå'}")
    print(f"   ADAUSDT Short: {'FIXED ‚úÖ' if ada_result else 'Still broken ‚ùå'}")
    
    all_pass = btc_result and eth_result and ada_result
    print(f"\nüèÜ OVERALL: {'ALL TESTS PASS - READY TO DEPLOY ‚úÖ' if all_pass else 'NEEDS MORE WORK ‚ùå'}")
    
    return all_pass

if __name__ == "__main__":
    run_tests()
